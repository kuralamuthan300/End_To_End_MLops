{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kural\\\\Desktop\\\\Projects\\\\End_To_End_MLops'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path(\"C:\\\\Users\\\\kural\\\\Desktop\\\\Projects\\\\End_To_End_MLops\\\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kural\\\\Desktop\\\\Projects\\\\End_To_End_MLops'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from software_defect_prediction.constants import *\n",
    "from software_defect_prediction.utils.common import *\n",
    "from software_defect_prediction.entity.config_entity import ModelEvaluationConfig\n",
    "from software_defect_prediction.config.configuration import ConfigurationManager\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import exception\n",
    "import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, matthews_corrcoef, cohen_kappa_score,\n",
    "    balanced_accuracy_score, hamming_loss, jaccard_score\n",
    ")\n",
    "import json\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "\n",
    "class Model_Evaulation():\n",
    "    def __init__(self,model_evaluation_config : ModelEvaluationConfig, model_parameters : dict ,predictor_col : str,mlflow_tracking : ConfigBox) -> None:\n",
    "        self.config = model_evaluation_config\n",
    "        self.predictor_col = predictor_col\n",
    "        self.model_parameters = model_parameters.MODEL_PARAMETERS.to_dict()\n",
    "        self.__mlflow_tracking = mlflow_tracking\n",
    "        self.model = None\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        \n",
    "    def prepare_and_load_files(self) -> None:\n",
    "        try :\n",
    "            \n",
    "            files_arr = [self.config.train_file,self.config.test_file,self.config.model_file]\n",
    "            \n",
    "            for file_name in files_arr:\n",
    "                source_file_path = Path(Path(self.config.source_file_path) / Path(file_name))\n",
    "                destination_file_path = Path(Path(self.config.root_dir) / Path(file_name))\n",
    "                if os.path.exists(destination_file_path):\n",
    "                    os.remove(destination_file_path)\n",
    "\n",
    "                shutil.copy(source_file_path,self.config.root_dir)\n",
    "            logger.info(Path(self.config.root_dir) / Path(self.config.train_file))\n",
    "            self.train_df = pd.read_csv(Path(self.config.root_dir) / Path(self.config.train_file))\n",
    "            self.test_df = pd.read_csv(Path(self.config.root_dir) / Path(self.config.test_file))\n",
    "            self.model = joblib.load(Path(Path(self.config.root_dir) / Path(\"random_forest_model.joblib\")))\n",
    "            logger.info(\"model, train and test data loaded successfully\")\n",
    "            \n",
    "        except exception as e:\n",
    "            logger.error(\"model, train and test data loding failed\")\n",
    "            raise(e)\n",
    "    \n",
    "    def setup_mlflow(self):\n",
    "        os.environ['MLFLOW_TRACKING_URI'] = self.__mlflow_tracking.MLFLOW_TRACKING_URI\n",
    "        os.environ['MLFLOW_TRACKING_USERNAME'] =  self.__mlflow_tracking.MLFLOW_TRACKING_USERNAME\n",
    "        os.environ['MLFLOW_TRACKING_PASSWORD'] = self.__mlflow_tracking.MLFLOW_TRACKING_PASSWORD\n",
    "    \n",
    "    def eval_metrics_classification(self, actual, pred):\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "        precision = precision_score(actual, pred, average='weighted')\n",
    "        recall = recall_score(actual, pred, average='weighted')\n",
    "        f1 = f1_score(actual, pred, average='weighted')\n",
    "        roc_auc = roc_auc_score(actual, pred, average='weighted', multi_class='ovr')\n",
    "        # cm = confusion_matrix(actual, pred)\n",
    "        mcc = matthews_corrcoef(actual, pred)\n",
    "        cohen_kappa = cohen_kappa_score(actual, pred)\n",
    "        balanced_acc = balanced_accuracy_score(actual, pred)\n",
    "        hamming = hamming_loss(actual, pred)\n",
    "        jaccard = jaccard_score(actual, pred, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            # 'confusion_matrix': cm,\n",
    "            'mcc': mcc,\n",
    "            'cohen_kappa': cohen_kappa,\n",
    "            'balanced_accuracy': balanced_acc,\n",
    "            'hamming_loss': hamming,\n",
    "            'jaccard_score': jaccard\n",
    "        }\n",
    "        \n",
    "    def log_into_mlflow(self) -> None:\n",
    "        \n",
    "        X_train = self.train_df.drop(columns=['id',self.predictor_col])\n",
    "        \n",
    "        X_test = self.test_df.drop(columns=['id',self.predictor_col])\n",
    "        y_test = self.test_df[self.predictor_col].astype(int)\n",
    "        \n",
    "        mlflow.set_tracking_uri(uri=self.__mlflow_tracking.MLFLOW_TRACKING_URI)\n",
    "        mlflow.set_experiment(self.__mlflow_tracking.MLFLOW_TRACKING_EXPERIMENT)\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            \n",
    "            y_pred = self.model.predict(X_test)\n",
    "            \n",
    "            model_perf_metrics = self.eval_metrics_classification(actual=y_test,pred=y_pred)\n",
    "            with open(Path(Path(self.config.root_dir) / Path(self.config.perf_metrics_file)), \"w\") as outfile: \n",
    "                json.dump(model_perf_metrics, outfile)\n",
    "            \n",
    "            mlflow.log_params(self.model_parameters)\n",
    "            for metric_name, value in model_perf_metrics.items():\n",
    "                mlflow.log_metric(metric_name, value)\n",
    "\n",
    "            signature = infer_signature(X_train, self.model.predict(X_train))\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=self.model,\n",
    "                signature=signature,\n",
    "                input_example=X_train,\n",
    "                artifact_path=\"software_defect_prediction\",\n",
    "                registered_model_name=\"RandomForestClassifier\",\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-01 21:37:32.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: config\\config.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-06-01 21:37:32.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: params.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-06-01 21:37:32.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: schema.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-06-01 21:37:32.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: credentials.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-06-01 21:37:32.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mcreate_directories\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcreated directory at: artifacts\u001b[0m\n",
      "\u001b[32m2024-06-01 21:37:32.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mcreate_directories\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcreated directory at: artifacts/model_evaluation\u001b[0m\n",
      "\u001b[32m2024-06-01 21:37:32.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_and_load_files\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1martifacts\\model_evaluation\\train_data.csv\u001b[0m\n",
      "\u001b[32m2024-06-01 21:37:32.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_and_load_files\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mmodel, train and test data loaded successfully\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "log_model() got an unexpected keyword argument 'artifacts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m model_eval\u001b[38;5;241m.\u001b[39mprepare_and_load_files()\n\u001b[0;32m      8\u001b[0m model_eval\u001b[38;5;241m.\u001b[39msetup_mlflow()\n\u001b[1;32m----> 9\u001b[0m model_eval\u001b[38;5;241m.\u001b[39mlog_into_mlflow()\n",
      "Cell \u001b[1;32mIn[21], line 101\u001b[0m, in \u001b[0;36mModel_Evaulation.log_into_mlflow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(metric_name, value)\n\u001b[0;32m     99\u001b[0m signature \u001b[38;5;241m=\u001b[39m infer_signature(X_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_train))\n\u001b[1;32m--> 101\u001b[0m mlflow\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39mlog_model(\n\u001b[0;32m    102\u001b[0m     sk_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    103\u001b[0m     signature\u001b[38;5;241m=\u001b[39msignature,\n\u001b[0;32m    104\u001b[0m     input_example\u001b[38;5;241m=\u001b[39mX_train,\n\u001b[0;32m    105\u001b[0m     artifacts\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftware_defect_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m     registered_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomForestClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: log_model() got an unexpected keyword argument 'artifacts'"
     ]
    }
   ],
   "source": [
    "config_manager = ConfigurationManager()\n",
    "model_evaluation_config = config_manager.get_model_evaluation_config()\n",
    "model_params = config_manager.get_model_params()\n",
    "predictor = config_manager.get_data_schema().TARGET_COLUMN.name\n",
    "\n",
    "model_eval = Model_Evaulation(model_evaluation_config,model_params,predictor,config_manager.get_mlflow_credentials())\n",
    "model_eval.prepare_and_load_files()\n",
    "model_eval.setup_mlflow()\n",
    "model_eval.log_into_mlflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
