{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kural\\\\Desktop\\\\Projects\\\\End_To_End_MLops'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path(\"C:\\\\Users\\\\kural\\\\Desktop\\\\Projects\\\\End_To_End_MLops\\\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kural\\\\Desktop\\\\Projects\\\\End_To_End_MLops'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from software_defect_prediction.constants import *\n",
    "from software_defect_prediction.utils.common import *\n",
    "from software_defect_prediction.entity.config_entity import ModelEvaluationConfig\n",
    "from software_defect_prediction.config.configuration import ConfigurationManager\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import exception\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, matthews_corrcoef, cohen_kappa_score,\n",
    "    balanced_accuracy_score, hamming_loss, jaccard_score\n",
    ")\n",
    "import json\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "\n",
    "class Model_Evaulation():\n",
    "    def __init__(self,model_evaluation_config : ModelEvaluationConfig, model_parameters : dict ,predictor_col : str,mlflow_tracking : ConfigBox) -> None:\n",
    "        self.config = model_evaluation_config\n",
    "        self.predictor_col = predictor_col\n",
    "        self.model_parameters = model_parameters.MODEL_PARAMETERS.to_dict()\n",
    "        self.__mlflow_tracking = mlflow_tracking\n",
    "        self.model = None\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        \n",
    "    def prepare_and_load_files(self) -> None:\n",
    "        try :\n",
    "            \n",
    "            files_arr = [self.config.train_file,self.config.test_file,self.config.model_file]\n",
    "            \n",
    "            for file_name in files_arr:\n",
    "                source_file_path = Path(Path(self.config.source_file_path) / Path(file_name))\n",
    "                destination_file_path = Path(Path(self.config.root_dir) / Path(file_name))\n",
    "                if os.path.exists(destination_file_path):\n",
    "                    os.remove(destination_file_path)\n",
    "\n",
    "                shutil.copy(source_file_path,self.config.root_dir)\n",
    "            logger.info(Path(self.config.root_dir) / Path(self.config.train_file))\n",
    "            self.train_df = pd.read_csv(Path(self.config.root_dir) / Path(self.config.train_file))\n",
    "            self.test_df = pd.read_csv(Path(self.config.root_dir) / Path(self.config.test_file))\n",
    "            self.model = joblib.load(Path(Path(self.config.root_dir) / Path(\"random_forest_model.joblib\")))\n",
    "            logger.info(\"model, train and test data loaded successfully\")\n",
    "            \n",
    "        except exception as e:\n",
    "            logger.error(\"model, train and test data loding failed\")\n",
    "            raise(e)\n",
    "    \n",
    "    def setup_mlflow(self):\n",
    "        os.environ['MLFLOW_TRACKING_URI'] = self.__mlflow_tracking.MLFLOW_TRACKING_URI\n",
    "        os.environ['MLFLOW_TRACKING_USERNAME'] =  self.__mlflow_tracking.MLFLOW_TRACKING_USERNAME\n",
    "        os.environ['MLFLOW_TRACKING_PASSWORD'] = self.__mlflow_tracking.MLFLOW_TRACKING_PASSWORD\n",
    "    \n",
    "    def eval_metrics_classification(self, actual, pred):\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "        precision = precision_score(actual, pred, average='weighted')\n",
    "        recall = recall_score(actual, pred, average='weighted')\n",
    "        f1 = f1_score(actual, pred, average='weighted')\n",
    "        roc_auc = roc_auc_score(actual, pred, average='weighted', multi_class='ovr')\n",
    "        # cm = confusion_matrix(actual, pred)\n",
    "        mcc = matthews_corrcoef(actual, pred)\n",
    "        cohen_kappa = cohen_kappa_score(actual, pred)\n",
    "        balanced_acc = balanced_accuracy_score(actual, pred)\n",
    "        hamming = hamming_loss(actual, pred)\n",
    "        jaccard = jaccard_score(actual, pred, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            # 'confusion_matrix': cm,\n",
    "            'mcc': mcc,\n",
    "            'cohen_kappa': cohen_kappa,\n",
    "            'balanced_accuracy': balanced_acc,\n",
    "            'hamming_loss': hamming,\n",
    "            'jaccard_score': jaccard\n",
    "        }\n",
    "        \n",
    "    def log_into_mlflow(self) -> None:\n",
    "        \n",
    "        X_train = self.train_df.drop(columns=['id',self.predictor_col])\n",
    "        \n",
    "        X_test = self.test_df.drop(columns=['id',self.predictor_col])\n",
    "        y_test = self.test_df[self.predictor_col].astype(int)\n",
    "        \n",
    "        mlflow.set_tracking_uri(uri=self.__mlflow_tracking.MLFLOW_TRACKING_URI)\n",
    "        mlflow.set_experiment(self.__mlflow_tracking.MLFLOW_TRACKING_EXPERIMENT)\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            \n",
    "            y_pred = self.model.predict(X_test)\n",
    "            \n",
    "            model_perf_metrics = self.eval_metrics_classification(actual=y_test,pred=y_pred)\n",
    "            # json.dump(model_perf_metrics,Path(self.config.root_dir)/Path(self.config.perf_metrics_file))\n",
    "            \n",
    "            mlflow.log_params(self.model_parameters)\n",
    "            for metric_name, value in model_perf_metrics.items():\n",
    "                mlflow.log_metric(metric_name, value)\n",
    "\n",
    "            signature = infer_signature(X_train, self.model.predict(X_train))\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=self.model,\n",
    "                artifact_path=\"iris_model\",\n",
    "                signature=signature,\n",
    "                input_example=X_train,\n",
    "                registered_model_name=\"RandomForestClassifier\",\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-01 15:18:55.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: config\\config.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-06-01 15:18:55.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: params.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-06-01 15:18:55.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: schema.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-06-01 15:18:55.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: credentials.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-06-01 15:18:55.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mcreate_directories\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcreated directory at: artifacts\u001b[0m\n",
      "\u001b[32m2024-06-01 15:18:55.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mcreate_directories\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcreated directory at: artifacts/model_evaluation\u001b[0m\n",
      "\u001b[32m2024-06-01 15:18:55.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_and_load_files\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1martifacts\\model_evaluation\\train_data.csv\u001b[0m\n",
      "\u001b[32m2024-06-01 15:18:56.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_and_load_files\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mmodel, train and test data loaded successfully\u001b[0m\n",
      "c:\\Users\\kural\\anaconda3\\envs\\jupyter_nb\\Lib\\site-packages\\_distutils_hack\\__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'RandomForestClassifier'.\n",
      "2024/06/01 15:19:32 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: RandomForestClassifier, version 1\n",
      "Created version '1' of model 'RandomForestClassifier'.\n"
     ]
    }
   ],
   "source": [
    "config_manager = ConfigurationManager()\n",
    "model_evaluation_config = config_manager.get_model_evaluation_config()\n",
    "model_params = config_manager.get_model_params()\n",
    "predictor = config_manager.get_data_schema().TARGET_COLUMN.name\n",
    "\n",
    "model_eval = Model_Evaulation(model_evaluation_config,model_params,predictor,config_manager.get_mlflow_credentials())\n",
    "model_eval.prepare_and_load_files()\n",
    "model_eval.setup_mlflow()\n",
    "model_eval.log_into_mlflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
