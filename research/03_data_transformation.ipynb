{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kural\\\\Desktop\\\\Projects\\\\End_To_End_MLops'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path(\"C:\\\\Users\\\\kural\\\\Desktop\\\\Projects\\\\End_To_End_MLops\\\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kural\\\\Desktop\\\\Projects\\\\End_To_End_MLops'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from software_defect_prediction.constants import *\n",
    "from software_defect_prediction.utils.common import *\n",
    "from software_defect_prediction.entity.config_entity import DataTransformationConfig\n",
    "from software_defect_prediction.config.configuration import ConfigurationManager\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import exception\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "class Data_Transformation():\n",
    "    def __init__(self,data_transformation_config : DataTransformationConfig,predictor_col : str) -> None:\n",
    "        self.config = data_transformation_config\n",
    "        self.predictor_col = predictor_col\n",
    "        self.input_df = None\n",
    "        \n",
    "    def prepare_and_load_files(self) -> None:\n",
    "        try :\n",
    "            destination_file_path = Path(Path(self.config.root_dir) / Path(self.config.input_file_name))\n",
    "            if os.path.exists(destination_file_path):\n",
    "                os.remove(destination_file_path)\n",
    "        \n",
    "            shutil.copy(self.config.source_file_path,self.config.root_dir)\n",
    "            \n",
    "            input_file_path = Path(Path(self.config.root_dir) / Path(self.config.input_file_name))\n",
    "            self.input_df = pd.read_csv(input_file_path)\n",
    "            \n",
    "            logger.info(\"input file loaded successfully\")\n",
    "        except exception as e:\n",
    "            logger.error(\"input file loading failed\")\n",
    "            raise(e)\n",
    "        \n",
    "    def tr_test_split_and_transform(self) -> None:\n",
    "        try :\n",
    "            input_df = self.input_df\n",
    "            predictor_col = self.predictor_col\n",
    "            X = input_df.drop(columns=[predictor_col])\n",
    "            y = input_df[predictor_col]\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y, shuffle=True)\n",
    "            logger.info(\"train test split completed\")\n",
    "\n",
    "            robust_scaler = RobustScaler().fit(X_train.drop(columns=\"id\"))\n",
    "\n",
    "            X_train_scaled = pd.DataFrame(robust_scaler.transform(X_train.drop(columns=\"id\")), columns=X_train.columns.drop('id'))\n",
    "            X_test_scaled = pd.DataFrame(robust_scaler.transform(X_test.drop(columns=\"id\")), columns=X_test.columns.drop('id'))\n",
    "\n",
    "            X_train_scaled['id'] = X_train['id'].values\n",
    "            X_test_scaled['id'] = X_test['id'].values\n",
    "\n",
    "            train_df = pd.concat([X_train_scaled, y_train.reset_index(drop=True)], axis=1)\n",
    "            test_df = pd.concat([X_test_scaled, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "            logger.info(\"train and test data scaling through robust scaler completed\")\n",
    "            \n",
    "            joblib.dump(robust_scaler,Path(self.config.root_dir/Path(\"robust_scaler.joblib\")))\n",
    "            train_df.to_csv(Path(self.config.root_dir/Path(\"train_data.csv\")), index=False)\n",
    "            test_df.to_csv(Path(self.config.root_dir/Path(\"test_data.csv\")), index=False)\n",
    "            \n",
    "            logger.info(f\"robust_scaler.joblib, train_data.csv, test_data.csv are saved to \",Path(self.config.root_dir/Path(\"train_data.csv\")))\n",
    "\n",
    "        except exception as e :\n",
    "            raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-28 22:24:59.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: config\\config.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-05-28 22:24:59.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: params.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-05-28 22:24:59.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mread_yaml\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1myaml file: schema.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m2024-05-28 22:24:59.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mcreate_directories\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcreated directory at: artifacts\u001b[0m\n",
      "\u001b[32m2024-05-28 22:24:59.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msoftware_defect_prediction.utils.common\u001b[0m:\u001b[36mcreate_directories\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcreated directory at: artifacts/data_transformation\u001b[0m\n",
      "\u001b[32m2024-05-28 22:25:00.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_and_load_files\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1minput file loaded successfully\u001b[0m\n",
      "\u001b[32m2024-05-28 22:25:00.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtr_test_split_and_transform\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mtrain test split completed\u001b[0m\n",
      "\u001b[32m2024-05-28 22:25:00.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtr_test_split_and_transform\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mtrain and test data scaling through robust scaler completed\u001b[0m\n",
      "\u001b[32m2024-05-28 22:25:04.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtr_test_split_and_transform\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mrobust_scaler.joblib, train_data.csv, test_data.csv are saved to \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config_manager = ConfigurationManager()\n",
    "dt_step = Data_Transformation(config_manager.get_data_transformation_config(),config_manager.get_data_schema().TARGET_COLUMN.name)\n",
    "dt_step.prepare_and_load_files()\n",
    "dt_step.tr_test_split_and_transform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
